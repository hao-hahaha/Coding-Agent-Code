import datetime
import json
import os
from typing import Any, AsyncIterator, Literal, NamedTuple, Self

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionToolParam
from pydantic import BaseModel, Field

from core.domain.documentation_section import DocumentationSection
from core.domain.feedback import Feedback
from core.domain.integration.integration_domain import Integration
from core.domain.version_environment import VersionEnvironment

from .extract_company_info_from_domain_task import Product


class BaseResult(BaseModel):
    tool_name: str = Field(
        description="The name of the tool call",
    )

    status: Literal["assistant_proposed", "user_ignored", "completed", "failed"] = Field(
        description="The status of the tool call",
    )


class BaseToolCallRequest(BaseModel):
    ask_user_confirmation: bool | None = Field(
        default=None,
        description="Whether the tool call should be automatically executed by on the frontend (ask_user_confirmation=false), or if the user should be prompted to run the tool call (ask_user_confirmation=true). Based on the confidence of the meta-agent in the tool call.",
    )


class ImprovePromptToolCallRequest(BaseToolCallRequest):
    agent_run_id: str | None = Field(
        default=None,
        description="The id (agent_runs.id) of the runs among the 'agent_runs' that is the most representative of what we want to improve in the 'agent_instructions'",
    )
    instruction_improvement_request_message: str = Field(
        description="The feedback on the agent run (what is wrong with the output of the run, what is the expected output, etc.).",
    )


class ImprovePromptToolCallResult(BaseResult, ImprovePromptToolCallRequest):
    pass


class EditSchemaStructureToolCallRequest(BaseToolCallRequest):
    edition_request_message: str | None = Field(
        default=None,
        description="The message to edit the agent schema with.",
    )


class EditSchemaDescriptionAndExamplesToolCallRequest(BaseToolCallRequest):
    description_and_examples_edition_request_message: str | None = Field(
        default=None,
        description="The message to edit the agent schema's fields description and examples with.",
    )


class AIEngineerAgentChatMessage(BaseModel):
    role: Literal["USER", "PLAYGROUND", "ASSISTANT"] = Field(
        description="The role of the message sender, 'USER' is the actual human user browsing the playground, 'PLAYGROUND' are automated messages sent by the playground to the agent, and 'ASSISTANT' being the assistant generated by the agent",
    )
    content: str = Field(
        description="The content of the message",
        examples=[
            "Thank you for your help!",
            "What is the weather forecast for tomorrow?",
        ],
    )

    tool_call: dict[str, Any] | None

    tool_call_status: Literal["assistant_proposed", "user_ignored", "completed", "failed"] | None


class AgentRun(BaseModel):
    id: str = Field(
        description="The id of the agent run",
    )
    model: str | None = Field(
        default=None,
        description="The model that was used to generate the agent output",
    )
    input: str | None = Field(
        default=None,
        description="The input of the agent, if no error occurred.",
    )
    output: str | None = Field(
        default=None,
        description="The output of the agent, if no error occurred.",
    )
    error: dict[str, Any] | None = Field(
        default=None,
        description="The error that occurred during the agent run, if any.",
    )
    raw_response: str | None = Field(
        description="The raw LLM completions that were made by the agent to produce the output",
    )

    class ToolCall(BaseModel):
        name: str
        input: dict[str, Any]

    tool_calls: list[ToolCall] | None = Field(
        default=None,
        description="The tool calls that were made by the agent to produce the output",
    )
    cost_usd: float | None = Field(
        default=None,
        description="The cost of the agent run in USD",
    )
    duration_seconds: float | None = Field(
        default=None,
        description="The duration of the agent in seconds",
    )
    user_evaluation: Literal["positive", "negative"] | None = Field(
        default=None,
        description="The user evaluation of the agent output",
    )


class CurrentAgentContext(BaseModel):
    class Agent(BaseModel):
        name: str
        slug: str
        schema_id: int
        description: str | None = None
        input_schema: dict[str, Any]
        output_schema: dict[str, Any]
        used_integration: Integration | None = None
        is_input_variables_enabled: bool = Field(
            default=False,
            description="Whether the agent is using input variables",
        )
        is_structured_output_enabled: bool = Field(
            default=False,
            description="Whether the agent is using structured output",
        )
        is_deployed: bool = Field(
            default=False,
            description="Whether the agent is deployed",
        )

    current_agent: Agent = Field(
        description="The current agent to use for the conversation",
    )

    latest_agent_run: AgentRun | None = Field(
        default=None,
        description="The latest agent run",
    )

    previous_agent_runs: list[AgentRun] | None = Field(
        default=None,
        description="The previous agent runs",
    )

    class AgentLifecycleInfo(BaseModel):
        class DeploymentInfo(BaseModel):
            class Deployment(BaseModel):
                deployed_at: datetime.datetime | None = Field(
                    default=None,
                    description="The date of the deployment",
                )
                deployed_by_email: str | None = Field(
                    default=None,
                    description="The email of the staff member who deployed the 'current_agent' version",
                )
                environment: VersionEnvironment | None = Field(
                    default=None,
                    description="The environment in which the 'current_agent' version is deployed ('dev', 'staging' or 'production')",
                )
                model_used: str | None = Field(
                    default=None,
                    description="The model used to run the 'current_agent' deployment",
                )
                last_active_at: datetime.datetime | None = Field(
                    default=None,
                    description="The date of the last run of the 'current_agent' deployment",
                )
                run_count: int | None = Field(
                    default=None,
                    description="The number of runs of the 'current_agent' deployment",
                )
                notes: str | None = Field(
                    default=None,
                    description="The notes of the 'current_agent' deployment, added by the staff member who created the deployed version",
                )

            deployments: list[Deployment] | None = Field(
                default=None,
                description="The list of deployments of the 'current_agent'",
            )

        deployment_info: DeploymentInfo | None = Field(
            default=None,
            description="The deployment info of the agent",
        )

        class FeedbackInfo(BaseModel):
            user_feedback_count: int | None = Field(
                default=None,
                description="The number of user feedbacks",
            )

            class AgentFeedback(BaseModel):
                created_at: datetime.datetime | None = None
                outcome: Literal["positive", "negative"] | None = None
                comment: str | None = None

                @classmethod
                def from_domain(cls, feedback: Feedback) -> Self:
                    return cls(
                        created_at=feedback.created_at,
                        outcome=feedback.outcome,
                        comment=feedback.comment,
                    )

            latest_user_feedbacks: list[AgentFeedback] | None = Field(
                default=None,
                description="The 10 latest user feedbacks",
            )

        feedback_info: FeedbackInfo | None = Field(
            default=None,
            description="The info related to the user feedbacks of the agent.",
        )

        class InternalReviewInfo(BaseModel):
            reviewed_input_count: int | None = Field(
                default=None,
                description="The number of reviewed inputs",
            )

        internal_review_info: InternalReviewInfo | None = Field(
            default=None,
            description="The info related to the internal reviews of the agent.",
        )

    agent_lifecycle_info: AgentLifecycleInfo | None = Field(
        default=None,
        description="The lifecycle info of the agent",
    )


class UserContext(BaseModel):
    class CompanyContext(BaseModel):
        company_name: str | None = None
        company_description: str | None = None
        company_locations: list[str] | None = None
        company_industries: list[str] | None = None
        company_products: list[Product] | None = None
        existing_agents_descriptions: list[str] | None = Field(
            default=None,
            description="The list of existing agents for the company",
        )

    company_context: CompanyContext = Field(
        description="The context of the company to which the conversation belongs",
    )


class AIEngineerAgentInput(BaseModel):
    current_datetime: datetime.datetime = Field(
        description="The current datetime",
    )

    chat_messages: list[AIEngineerAgentChatMessage] = Field(
        description="The list of messages in the conversation, the last message being the most recent one",
    )

    user_programming_language: str | None = Field(
        default=None,
        description="The programming language of the user",
    )

    user_code_extract: str | None = Field(
        default=None,
        description="The code extract of the user",
    )

    current_agent_context: CurrentAgentContext | None = Field(
        description="The context of the current agent",
    )

    user_context: UserContext | None = Field(
        description="The context of the user",
    )

    workflowai_documentation_sections: list[DocumentationSection] = Field(
        description="The relevant documentation sections of the WorkflowAI platform, which this agent is part of",
    )

    available_hosted_tools_description: str = Field(
        description="The description of the available hosted tools, that can be potientially added to the agent 'messages' in order to improve the agent's output.",
    )

    class Model(BaseModel):
        id: str = Field(
            description="The id of the model",
        )
        name: str
        is_supported_for_agent: bool = Field(
            description="Whether the model is supported for the current agent",
        )
        is_not_supported_reason: str | None = Field(
            default=None,
            description="The reason why the model is not supported for the current agent",
        )
        is_default: bool = Field(
            default=False,
            description="Whether the model is one of the default models on the WorkflowAI platform",
        )
        is_latest: bool = Field(
            default=False,
            description="Whether the model is the latest model in its family",
        )
        quality_index: int = Field(
            description="The quality index that quantifies the reasoning abilities of the model",
        )
        quality_index_ranking: int = Field(
            description="The quality index ranking of the model, 1 being the smartest",
        )
        speed_index: int = Field(
            description="The speed index that quantifies the response speed of the model",
        )
        speed_index_ranking: int = Field(
            description="The speed index ranking of the model, 1 being the fastest",
        )
        context_window_tokens: int = Field(
            description="The context window of the model in tokens",
        )
        supports_structured_output: bool = Field(
            description="Whether the model supports structured output",
        )

        estimate_cost_per_thousand_runs_usd: float | None = Field(
            default=None,
            description="The estimated cost per thousand runs in USD",
        )
        cost_ranking: int = Field(
            description="The cost ranking of the model, 1 being the cheapest",
        )

    available_models: list[Model] = Field(
        description="The models currently available in WorkflowAI",
    )


class AIEngineerAgentOutput(BaseModel):
    assistant_answer: str | None = Field(
        default=None,
        description="The content of the answer message from the meta-agent",
    )

    improvement_instructions: str | None = Field(
        default=None,
        description="Instructions on how to improve the current agent version's messages, if any.",
    )

    class NewTool(BaseModel):
        name: str
        description: str
        parameters: dict[str, Any]

    new_tool: NewTool | None = Field(
        default=None,
        description="The new tool to add to the current agent, if any",
    )

    edit_schema_structure_request: EditSchemaStructureToolCallRequest | None = Field(
        default=None,
        description="The schema structure editing request, if any",
    )

    edit_schema_description_and_examples_request: EditSchemaDescriptionAndExamplesToolCallRequest | None = Field(
        default=None,
        description="The schema description and examples editing request, if any",
    )


class ParsedToolCall(NamedTuple):
    """Result of parsing a tool call from the OpenAI streaming response."""

    improvement_instructions: str | None = None
    tool_name: str | None = None
    tool_description: str | None = None
    tool_parameters: dict[str, Any] | None = None
    edit_schema_structure_request: EditSchemaStructureToolCallRequest | None = None
    edit_schema_description_and_examples_request: EditSchemaDescriptionAndExamplesToolCallRequest | None = None


def parse_tool_call(tool_call: Any) -> ParsedToolCall:
    """Parse a tool call and return the extracted data.

    Returns a ParsedToolCall with the parsed tool data. Fields are populated based on tool type:
    - update_version_messages: updated_version_messages, example_input
    - create_custom_tool: tool_name, tool_description, tool_parameters
    - edit_schema_structure: edit_schema_structure_request
    - edit_schema_description_and_examples: edit_schema_description_and_examples_request
    """
    if not tool_call.function or not tool_call.function.arguments:
        return ParsedToolCall()

    function_name = tool_call.function.name
    arguments = json.loads(tool_call.function.arguments)

    if function_name == "update_version_messages":
        return ParsedToolCall(
            improvement_instructions=arguments["improvement_instructions"],
        )

    if function_name == "create_custom_tool":
        return ParsedToolCall(
            tool_name=arguments["name"],
            tool_description=arguments["description"],
            tool_parameters=arguments["parameters"],
        )

    if function_name == "edit_output_schema_structure":
        return ParsedToolCall(
            edit_schema_structure_request=EditSchemaStructureToolCallRequest(
                edition_request_message=arguments.get("edition_request_message"),
                ask_user_confirmation=arguments.get("ask_user_confirmation"),
            ),
        )

    if function_name == "edit_output_schema_description_and_examples":
        return ParsedToolCall(
            edit_schema_description_and_examples_request=EditSchemaDescriptionAndExamplesToolCallRequest(
                description_and_examples_edition_request_message=arguments.get(
                    "description_and_examples_edition_request_message",
                ),
                ask_user_confirmation=arguments.get("ask_user_confirmation"),
            ),
        )

    return ParsedToolCall()


AI_ENGINEER_INSTRUCTIONS = """
You are the WorkflowAI AI Engineer agent. Your role is to help users build, improve, and debug their WorkflowAI agents directly from their IDE (Cursor, Windsurf, etc.). You make users succeed in the WorkflowAI platform by helping them create performant and reliable agents.

The discussion you are having with the user happens in their IDE/codebase context, where they are writing code to create or integrate WorkflowAI agents. Since you operate outside the WorkflowAI UI, you should provide relevant links when appropriate (e.g., to the playground, agent runs, documentation).

<current_datetime>
Current datetime is: {{current_datetime}}
</current_datetime>

<user_code>
The user is using the following programming language: {{user_programming_language}}
The code editor agent has provided the following code extract:

{{user_code_extract}}

{% if current_agent_context %}
IMPORTANT: In case of discrepancies between the user code extract and the current_agent_context, use the user code extract as the source of truth.
{% endif %}

</user_code>

{% if current_agent_context %}
The agent the user is currently working on is:
<current_agent>
{{current_agent_context}}
</current_agent>

It's also very important that you check the 'latest_agent_run' to see the latest agent runs that the user has made, directly from the code.
Pay attention to the temporality of those runs, with newer runs being at the beginning of the agent runs array.
If the first run in the array use Claude models, and the second one is GPT, that means the user has switched to Claude. If there are back and forth between models, consider that the user has tried cloud but went back to GPT.
{% else %}
The user may be creating a new agent or working on an agent that hasn't been registered in WorkflowAI yet. In this case, guide them through creating their first WorkflowAI agent with best practices.
{% endif %}


<other_context>
{{other_context}}
</other_context>

<code_blocks_guidelines>
When returning code blocks, always make sure the code block is customized to the user's specific context—for example, the model it uses or the name of its agent ('current_agent.slug').
When returning code blocks, always make sure to just return the part that is to be updated and do not return a full working code block that is too lengthy.
When returning code blocks, do not use diff formate (+, -, etc.) because the user will not be able to copy paste the code block effectively.
Inside returning code blocks do not forget to escape double quotes: \" , same for triple quotes: \"\"\"
When the current_agent.is_structured_output_enabled is false, you can't include a reponse format in the code block, since this is not what is currently used by the agent.
NEVER truncate a lines of code, either write the full line or omit the line and say '# Your existing...'.
When returning code blocks, always return the smallest chunk possible that really highlight what needs to be changed by the user, from the previous "messages". It's better to return three small code snippets that really highlight what needs to be changed rather than a big one where it's harder for the user to find what to change.
I repeat, you need to consider the code updates the user has made in the previous "messages" and return the smallest chunk possible that really highlight what needs to be changed. Do not repeat code that has not changed from previous messages.
Please be VERY careful NOT including comments in another language format ex: DO NOT USE '/* */' if 'current_agent.used_integration.programming_language==python' and DO NOT USE '#' if 'current_agent.used_integration.programming_language==typescript'.
Do NOT provider example for other languages / integration other that the one defined in 'current_agent.used_integration' and 'integration_documentation'.
You must return the code snippet wrapped in markdown code blocks with the appropriate language specified. For example:
```python
# Python code
```
or
```typescript
// TypeScript code
```
</code_blocks_guidelines>


<user_communication_guidelines>
IMPORTANT: When communicating with users, always use natural, user-friendly language:
- Never mention internal tool names (e.g., "I will use update_version_messages")
- Instead use descriptive language (e.g., "I will update your agent's messages", "I will modify your output schema", "I will run your agent on different models")
- Keep explanations focused on what you're doing for the user, not the technical implementation
</user_communication_guidelines>

<workflowai_user_journey>
You must always strive to help using the full capabilities of WorkflowAI.

Typical checklist for an optimal user journey:
- creating a new agent
- integrating the agent in the codebase
- running the agent
- trying the agent on different models
- adding input variables for better observability, reusability, benchmarking and deployments
- activate structured output for better output quality
- deploying the agent to an environment (dev, staging, production) in order to improve the agent 'online' without needind code changes
- monitoring the agent's performance, using feebacks, benchmark new models, deploy new versions, etc.
</workflowai_user_journey>

<factors_impacting_agent_performance>
You must always strive to help the user improve its 'current_agent' performance.
Several factors impact an agent behaviour and performance, here are the most common ones (and how to enhance those factors):

- The agent's messages: having unclear, missing or incorrect messages is a common reason for an agent to fail. See <improving_agent_messages> for more details.
- The agent's input and output schemas: having an incomplete, malformed or unnecessarily complex schema is a common reason for an agent to fail. See <improving_agent_input_and_output_schemas> for more details.
- The agent's tools: missing or wrong tools is a common reason for an agent to fail. See <current_agent_tool_capabilities> for more details.
- Wrong model is used: the agent's performance is impacted by the model used. See <running_agent_on_different_models> for more details.
- Other errors, ex generation errors, etc. See <error_analysis> for more details.
</factors_impacting_agent_performance>

<creating_new_agents>
When the user is creating a new agent (no current_agent in context):
- Provide a complete, self-contained, runnable example
- Include all imports, API key setup, and configuration
- Demonstrate best practices: input variables, structured output, proper error handling
- Include comments explaining WorkflowAI-specific features
- If you know the purpose of the agent, tailor the example to that use case
- If purpose is unknown, create a simple but useful example (e.g., text summarization, data extraction)

Example structure for new agents:
1. Imports and setup
2. Define input/output schemas (if using structured output)
3. Create messages with input variables
4. Initialize client with WorkflowAI API
5. Make completion request with proper configuration
6. Handle response and errors
7. Example usage with test input
</creating_new_agents>

<schema_editing_rules>
IMPORTANT distinctions for schema editing:
- INPUT SCHEMA EDITS: Always use 'update_version_messages' (input variables/definitions are in version messages)
- OUTPUT SCHEMA EDITS:
{% if agent_has_output_schema %}
  - Agent has structured generation activated: use 'edit_output_schema_structure' or 'edit_output_schema_description_and_examples'
{% else %}
  - Agent does NOT have structured generation activated: use 'update_version_messages'
{% endif %}

IMPORTANT: Before calling any of these tools, the user must clearly explain what they want to change. Only trigger these tools when the user has provided specific modification requirements.

COMMUNICATION RULE: Never mention tool names directly to users (e.g., "I will use update_version_messages"). Instead, use natural language (e.g., "I will update your agent's messages" or "I will modify your output schema").
</schema_editing_rules>

<input_variables>
{% if agent_is_using_input_variables %}
- Agent is using input variables.
- Any modifications to input variables or input schema MUST be done through 'update_version_messages' since they are embedded in the version messages.
{% else %}
- Agent is NOT using input variables. so if the user is asking to update the input variables you must use the 'update_version_messages' tool and also suggest the user to switch to input variables.

Also suggest the user to switch to input variables any time you find relevant.
{% endif %}
</input_variables>

<structured_output>
{% if agent_has_output_schema %}
- Agent is using structured output, so if the user is asking to update the output structure you must use the 'edit_output_schema_structure' tool or 'edit_output_schema_description_and_examples' tool.
- IMPORTANT: Only OUTPUT schema edits use these tools. INPUT schema edits must always use 'update_version_messages'.
{% else %}
- Agent is NOT using structured output yet so if the user is asking to update the output structure you must use the 'update_version_messages' tool and also suggest the user to switch to structured output.
- IMPORTANT: INPUT schema edits must always use 'update_version_messages' regardless of structured output status.

Also suggest the user to switch to structured output any time you find relevant.
{% endif %}
</structured_output>

<deployments>
{% if agent_is_deployed %}
<current_deployments>
{{deployments_str}}
</current_deployments>


- Agent is deployed, so if the user is trying out other models, etc. you must remind the user to deploy the new version to its environment. See <current_deployments> for more details about current deployments.
{% else %}
- Agent is NOT deployed yet so if the user is asking to update the agent you must use the 'update_version_messages' tool and also suggest the user to deploy the agent.

Also suggest the user to deploy the agent to an environment (dev, staging, production) any time you find relevant.
{% endif %}

Always double check the <current_deployments> because users can get confused about their deployments. See <current_deployments> as the source of truth.
</deployments>


<running_agent_on_different_models>
# In case the user enquires a about testing new models:
The reason to use different models is mostly: having better performing models, cheaper models or faster models.

Your answer MUST include:
- a brief explaination that one of the benefits of WorkflowAI is the ability to use different models from different providers to optimize either performance, cost or latency of agents.
- then you MUST only pass to the user the suggested models string in the code block, ex: model="MODEL_NAME_PREFIX_PLACEHOLDER<agent_name>/<suggested_model_name>". No other code block is needed. Ex: "To try out Claude 3.7 Sonnet, you can simply replace your existing model with: model="MODEL_NAME_PREFIX_PLACEHOLDERagent-name/claude-3-7-sonnet-20250219", (add a comma at the end of the line, to allow the user to copy paste it easily in his code).
- You MUST also offer the user to test different models by either: 1) updating their code with the new model, or 2) providing a link to test in the WorkflowAI playground where they can use 'run_agent_on_model'
- The actual model picking will depend on the user's request in 'chat_messages'. If now specific criterias are suggested, you can pick one "smart" model and one "cheap" model as mentioned above.

You can use the 'quality_index_ranking' and 'cost_ranking' fields in to quickly find the smartest and cheapest models. But ALWAYS recommend models that are supported by the agent (check 'is_supported_for_agent')
</running_agent_on_different_models>

<setting_up_input_variables>
# In case the user enquires a about input variables:

Use the 'suggested_messages_with_input_variables' and 'suggested_input_variables_example'.
Your answer must include:
- a brief rationale (100 words max.) of why using input variables is a good idea (clearer separation between the agent's instructions and the data it uses, better observability, enabled benchmarking and deployments), based on the documentation in 'workflowai_documentation_sections' and 'integration_documentation'
- in a first code block: all the messages from 'suggested_messages_with_input_variables'. Optionally define the messages in separate variable if the messages are lengthy.
- in a second code block: the part of the code where the updated messages are injected in the completion request. Make sure all the messages are used.
- in the second code block: the part of the code that shows how to pass the input variables in the completion request (with "extra_body": {"input": "..."} for OpenAI Python examples, WARNING OpenAI JS / TS does not support "extra_body", "input" needs to be passed in the top level of the completion request) AND '// @ts-expect-error input is specific to the WorkflowAI implementation' needs to be added if the code is in TS.

Your answer must NOT include:
- the parts where the user is setting its API keys
- the initialization of the client (ex: client=openai.OpenAI())
- do not talk about deployments at this stage
- any other content
</setting_up_input_variables>


<setting_up_structured_output>
# In case the user enquires a about structured output:
Your answer MUST include, different code blocks that show the following:
- a brief explanation (50 words max.) of why you are stuctured output is useful, based on the documentation in 'workflowai_documentation_sections' and 'integration_documentation' and the user context
- 'suggested_output_class_code' that shows the output class to use, including eventual description and examples.
- the messages MUST still contain input variables {% raw %} {{example_variable}} {% endraw %}
- pass the right response_format in the completion request
- completion client MUST ALWAYS be: COMPLETION_CLIENT_PLACEHOLDER
- the "messages" without the parts that are not needed anymore for structured generation (see: 'suggested_instructions_parts_to_remove') but DO NOT REMOVED INPUT VARIABLES if they were present before in the messages, since those are also needed for the structured output
- IMPORTANT: OpenAI SDK must be used with 'client.beta.chat.completions.parse' and NOT 'client.chat.completions.create' when using structured output.

Your answer must NOT include:
- the parts where the user is setting its API keys
- the initialization of the client (ex: client=openai.OpenAI())
- do not talk about deployments at this stage
- DO NOT REMOVED INPUT VARIABLES, neither from the 'messages' (in double curly braces), nor from from the completion request (ex: extra_body: {"input": "..."}, ,'input', ex.). Input variables are still needed for, even with the structured output.
- You MUST end your message with the 'setup_structured_output_assistant_proposal' in this cases
</setting_up_structured_output>

<setting_up_deployment>
# In case the user enquires a about deployments:
Check in the 'agent_lifecycle_info.deployment_info.deployments' to see if the 'current_agent' has already been deployed before answering.

You answer MUST include:
- Before talking about code update explains about how to deploy the agent based on the docs (200 words max.) in 'features/deployments.md'
- Add a link to https://docs.workflowai.com/features/deployments for the user to read more about deployments.
- Then, you can talk about the model parameter update needed:  MODEL_NAME_PREFIX_PLACEHOLDER<current_agent.slug>/#<current_agent.schema_id>/<deployment env (production, staging, dev)>
ex: model="MODEL_NAME_PREFIX_PLACEHOLDERmy-agent/#1/production" You can explain the format above to the user: (model="MODEL_NAME_PREFIX_PLACEHOLDERmy-agent/#1/production")
{% if is_using_version_messages %}
- A Note that the 'messages' array will be empty if the when using deployments because the messages are registered in the WorkflowAI deployment. So user can pass messages=[] but NOT OMITTED. Refer to the 'integration_documentation' for specifics for the integration used.
You can explain to the user in comment that messages can be empty because the messages static parts are stored in the WorkflowAI deployment.
{% endif %}

You answer MUST NOT INCLUDE:
- A repetition of the whole code from previous answers. You ONLY need to show the "model=..." parameters and the "messages=[]".
</setting_up_deployment>


<improving_agent_messages>
The messages (especially the system message and the first user message if any) explain the agent how to behave and how to generate its output, based on the input.
Having unclear, missing or incorrect messages is a common reason for an agent to fail.
Example for missing instructions: an agent that summarizes a 'source_text', the user wants bullet points 'summary' in output, but the messages are not mentioning this requirement. You need to run the 'update_version_messages' with the update messages.
When you recommend messages update, always do so by calling the 'update_version_messages'.

IMPORTANT:
- Input schema modifications must ALWAYS be done through 'update_version_messages' because input variables and input schema definitions are contained within the version messages
- Only call 'update_version_messages' when the user has explicitly described what changes they want to make to the agent's messages or input schema
</improving_agent_messages>

<improving_agent_input_and_output_schemas>
There are two distinct types of schema modifications that require different approaches:

INPUT SCHEMA MODIFICATIONS:
- Input schema changes must ALWAYS be done using 'update_version_messages'
- This is because input variables and input schema definitions are embedded within the agent's messages
- Examples: changing input field names, adding/removing input fields
- Example: "I want to change the input field from 'source_text' to 'document_content'" → use 'update_version_messages'
- ONLY trigger when user explicitly requests input schema changes

OUTPUT SCHEMA MODIFICATIONS:
- Output schema changes depend on whether the agent uses structured generation:

{% if agent_has_output_schema %}
For agents WITH structured generation activated:
- Use 'edit_output_schema_structure' for structural changes (adding/removing fields, changing field types, field names)
- Use 'edit_output_schema_description_and_examples' for updating field descriptions and examples
- Example for missing field in output: "I want to add the 'summary' field to the output of the agent" → use 'edit_output_schema_structure'
- Example for field description update: "I want to improve the description of the confidence field" → use 'edit_output_schema_description_and_examples'
- ONLY trigger when user explicitly requests output schema changes
{% else %}
For agents WITHOUT structured generation activated:
- Use 'update_version_messages' to modify output requirements in the messages
- Example: "I want the agent to generate a 'summary' field in the output" → use 'update_version_messages'
- ONLY trigger when user explicitly requests output changes
{% endif %}
</improving_agent_input_and_output_schemas>


<current_agent_tool_capabilities>
The 'available_tools_description' field in input contains a description of the tools that can be used to improve the agent's output (web-browser, web search, etc.).
Keep in mind that the LLMs that power the current_agent, can't access the internet on their own, they can't get real time data (weather, news, etc.). nor information that did not exist when the agent was trained (often months or years ago).

<hosted_tools>
You can enhance the agent capabilities by using hosted tools that will run inside the workflow AI platform when the user makes an agent run, those tools are detailled in '<available_hosted_tools_description>' below. Hosted tools can directly be added to the agent's message, so to add an hosted tool, you can call the 'update_version_messages' tool call with the new message containing the hosted tool (@....)
<available_hosted_tools_description>
{{available_hosted_tools_description}}
</available_hosted_tools_description>
</hosted_tools>

<custom_tools>
In case the tools are not enough to endure the task the agent must do, you can propose the user to add a custom tool by making a 'create_custom_tool' tool call. In this case, the user will have to implement the actual tool in their codebase.
</custom_tools>
</current_agent_tool_capabilities>

<error_analysis>
When a run as an 'error', that means the output could not have been parsed. You can analyse the 'raw_response' of the run to understand why generation has failed. A typical error is the models outputing "properties": ... (similar to a JSON schema) instead of the expected JSON object.
This is a common issue with small and low quality index models. In those cases, you must recommend the user to use a models that support structured outputs see models with "supports_structured_output": true, and have similar 'price_ranking' and 'quality_index_ranking'.
</error_analysis>

<overall_discussion_flow>
Be mindful of subjects that are "over" in the messages, and those who are current. You do not need to answer messages that were already answered. Avoid proposing again the same tool call or similar ones if previous tool calls are 'user_ignored'.
Be particularly mindful of the past tool calls that were made. Analyze the tool calls status ("assistant_proposed", "user_ignored", "completed", "failed") to assess the relevance of the tool calls.
If the latest tool call in the message is "user_ignored", it means that the tool call is not relevant to the user's request, so you should probably offer something else as a next step.
If the latest tool call in the message is "completed", you should most of the time ask the user if there is anything else you can do for them without proposing any tool call, unless you are sure that the improvement did not go well. Do not repeat several tool calls of the same type in a row, except if the user asks for it or if the original problem that was expressed by the user is not solved. Keep in mind that you won't be able to solve all problems on all models and sometimes you just have to accept that some models doesn't perform very well on the 'current_agent' so you must spot the models that work well and advise the user to use those instead (unless a user really want to use a specific model, for example for cost reasons). If you found at least one model that works well, you must offer the user to use this model for the 'current_agent'. Indeed, if none of the models among the three selected models works well, you can either make another round of improving the version messages / schema or offer to try different models with higher 'quality_index' using the 'run_agent_on_model' tool call.
</overall_discussion_flow>

Available models for the agent are:
{{available_models_str}}

The conversation with the user is below, pay close attention to the whole discussion flow in order to craft the most pertinent answer.
{{chat_messages}}"""

TOOL_DEFINITIONS: list[ChatCompletionToolParam] = [
    {
        "type": "function",
        "function": {
            "name": "update_version_messages",
            "description": "Update the messages of the current agent version by providing instructions for improvement. This tool must ALWAYS be used for any input schema modifications since input variables and input schema definitions are embedded within the version messages. Only call this tool when the user has explicitly described what changes they want to make. When calling this tool, tell the user in natural language what you're doing (e.g., 'I will update your agent's messages') rather than mentioning the tool name.",
            "parameters": {
                "type": "object",
                "properties": {
                    "improvement_instructions": {
                        "type": "string",
                        "description": "Instructions on how to improve the current agent version's messages. This will be passed to a specialized agent.",
                    },
                },
                "required": [
                    "improvement_instructions",
                ],
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
    {
        "type": "function",
        "function": {
            "name": "create_custom_tool",
            "description": "Adds a custom tool to the current agent to enhance its capabilities. When proposing a tool, be mindful of what is the realistic needed parameters, for example for a translation tool, the parameters will be 'source_text' and 'target_language'.",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {"type": "string", "description": "The name of the tool to add."},
                    "description": {
                        "type": "string",
                        "description": "The description of the tool to add.",
                    },
                    "parameters": {
                        "type": "object",
                        "description": "The parameters of the tool to add.",
                    },
                },
                "required": ["name", "description", "parameters"],
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
    {
        "type": "function",
        "function": {
            "name": "edit_output_schema_structure",
            "description": "Edit the structural aspects of the agent's OUTPUT schema ONLY including fields, field names, field types, etc. IMPORTANT: This tool is exclusively for output schema modifications - input schema changes must use 'update_version_messages'. When calling this tool, tell the user in natural language what you're doing (e.g., 'I will modify your output schema structure') rather than mentioning the tool name.",
            "parameters": {
                "type": "object",
                "properties": {
                    "edition_request_message": {
                        "type": "string",
                        "description": "The message describing the structural changes to make to the agent OUTPUT schema (e.g., 'Add a new field called confidence with type number', 'Remove the optional field description', 'Make the field email required').",
                    },
                },
                "required": ["edition_request_message"],
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
    {
        "type": "function",
        "function": {
            "name": "edit_output_schema_description_and_examples",
            "description": "Edit the descriptions and examples of fields in the agent's OUTPUT schema ONLY without changing the structure. IMPORTANT: This tool is exclusively for output schema modifications - input schema changes must use 'update_version_messages'. When calling this tool, tell the user in natural language what you're doing (e.g., 'I will improve your output schema descriptions') rather than mentioning the tool name.",
            "parameters": {
                "type": "object",
                "properties": {
                    "description_and_examples_edition_request_message": {
                        "type": "string",
                        "description": "The message describing the description and example changes to make to the agent OUTPUT schema fields (e.g., 'Update the description of the name field to be more specific', 'Add examples to the email field', 'Improve the description of the confidence field').",
                    },
                },
                "required": ["description_and_examples_edition_request_message"],
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
]


async def ai_engineer_agent(
    input: AIEngineerAgentInput,
    instructions: str,
) -> AsyncIterator[AIEngineerAgentOutput]:
    client = AsyncOpenAI(
        api_key=os.environ["WORKFLOWAI_API_KEY"],
        base_url=f"{os.environ['WORKFLOWAI_API_URL']}/v1",
    )

    response = await client.chat.completions.create(
        model="ai-engineer-agent/claude-sonnet-4-20250514",
        messages=[
            {"role": "system", "content": instructions},
            {
                "role": "user",
                "content": "Your answer is:",
            },
        ],
        stream=True,
        temperature=0.0,
        extra_body={
            "input": {
                "current_datetime": input.current_datetime.isoformat(),
                "user_programming_language": input.user_programming_language,
                "user_code_extract": input.user_code_extract,
                "current_agent_context": input.current_agent_context.model_dump_json(indent=4, exclude_none=True)
                if input.current_agent_context
                else "",
                "agent_is_using_input_variables": input.current_agent_context
                and input.current_agent_context.current_agent
                and input.current_agent_context.current_agent.is_input_variables_enabled
                or "",
                "agent_has_output_schema": input.current_agent_context
                and input.current_agent_context.current_agent
                and input.current_agent_context.current_agent.is_structured_output_enabled
                or "",
                "agent_is_deployed": input.current_agent_context
                and input.current_agent_context.current_agent
                and input.current_agent_context.current_agent.is_deployed
                or "",
                "chat_messages": "\n".join(
                    [message.model_dump_json(indent=4, exclude_none=True) for message in input.chat_messages],
                ),
                "available_models_str": "\n".join(
                    [model.model_dump_json(indent=4, exclude_none=True) for model in input.available_models],
                ),
                "available_hosted_tools_description": input.available_hosted_tools_description,
                "other_context": "{% raw %}"
                + input.model_dump_json(
                    indent=4,
                    exclude=dict(
                        current_datetime=True,
                        current_agent_context=True,
                        chat_messages=True,
                        available_hosted_tools_description=True,
                        available_models=True,
                    ),
                )
                + "{% endraw %}",
            },
            "use_cache": "never",
        },
        tools=TOOL_DEFINITIONS,
    )

    async for chunk in response:
        # Parse tool calls if present
        parsed_tool_call = ParsedToolCall()
        if chunk.choices[0].delta.tool_calls:
            tool_call = chunk.choices[0].delta.tool_calls[0]
            parsed_tool_call = parse_tool_call(tool_call)

        yield AIEngineerAgentOutput(
            assistant_answer=chunk.choices[0].delta.content,
            improvement_instructions=parsed_tool_call.improvement_instructions,
            new_tool=AIEngineerAgentOutput.NewTool(
                name=parsed_tool_call.tool_name,
                description=parsed_tool_call.tool_description,
                parameters=parsed_tool_call.tool_parameters,
            )
            if parsed_tool_call.tool_name and parsed_tool_call.tool_description and parsed_tool_call.tool_parameters
            else None,
            edit_schema_structure_request=parsed_tool_call.edit_schema_structure_request,
            edit_schema_description_and_examples_request=parsed_tool_call.edit_schema_description_and_examples_request,
        )
