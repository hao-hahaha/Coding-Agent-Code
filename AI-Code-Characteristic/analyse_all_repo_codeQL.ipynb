{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8d79bd",
   "metadata": {},
   "source": [
    "# Read in PR full data (including source file in each pull request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pr_full_data_0 = pd.read_json(\n",
    "    \"pr_full_data_shard_0.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "pr_full_data_1 = pd.read_json(\n",
    "    \"pr_full_data_shard_1.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "pr_full_data_2 = pd.read_json(\n",
    "    \"pr_full_data_shard_2.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "pr_full_data_3 = pd.read_json(\n",
    "    \"pr_full_data_shard_3.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "pr_full_data_4 = pd.read_json(\n",
    "    \"pr_full_data_shard_4.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "pr_full_data_5 = pd.read_json(\n",
    "    \"pr_full_data_shard_5.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "pr_full_data_6 = pd.read_json(\n",
    "    \"pr_full_data_shard_6.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "pr_full_data_7 = pd.read_json(\n",
    "    \"pr_full_data_shard_7.jsonl\",\n",
    "    lines=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced21fd0",
   "metadata": {},
   "source": [
    "## handle errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df0 = pr_full_data_0[pr_full_data_0['error'].notna()]\n",
    "error_df0['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df1 = pr_full_data_1[pr_full_data_1['error'].notna()]\n",
    "error_df1['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca033a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df2 = pr_full_data_2[pr_full_data_2['error'].notna()]\n",
    "error_df2['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b94039",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df3 = pr_full_data_3[pr_full_data_3['error'].notna()]\n",
    "error_df3['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5729e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df4 = pr_full_data_4[pr_full_data_4['error'].notna()]\n",
    "error_df4['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea481b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df5 = pr_full_data_5[pr_full_data_5['error'].notna()]\n",
    "error_df5['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df6 = pr_full_data_6[pr_full_data_6['error'].notna()]\n",
    "error_df6['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392004ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df7 = pr_full_data_7[pr_full_data_7['error'].notna()]\n",
    "error_df7['error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae451096",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_full_data = pd.concat(\n",
    "    [\n",
    "        pr_full_data_0,\n",
    "        pr_full_data_1,\n",
    "        pr_full_data_2,\n",
    "        pr_full_data_3,\n",
    "        pr_full_data_4,\n",
    "        pr_full_data_5,\n",
    "        pr_full_data_6,\n",
    "        pr_full_data_7,\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "error_full_data = pd.concat(\n",
    "    [\n",
    "        error_df0,\n",
    "        error_df1,\n",
    "        error_df2,\n",
    "        error_df3,\n",
    "        error_df4,\n",
    "        error_df5,\n",
    "        error_df6,\n",
    "        error_df7,\n",
    "\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_full_data\n",
    "# there are 121 pull request is not able to access due to deletion and access blocked reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_error_df = pr_full_data[pr_full_data['error'].notna()].copy()\n",
    "KEY_COLS = ['html_url']\n",
    "pr_error_keys = pr_error_df[KEY_COLS]\n",
    "error_keys = error_full_data[KEY_COLS]\n",
    "\n",
    "diff_1 = pr_error_keys.merge(\n",
    "    error_keys,\n",
    "    on=KEY_COLS,\n",
    "    how='left',\n",
    "    indicator=True\n",
    ").query('_merge == \"left_only\"')\n",
    "\n",
    "diff_2 = error_keys.merge(\n",
    "    pr_error_keys,\n",
    "    on=KEY_COLS,\n",
    "    how='left',\n",
    "    indicator=True\n",
    ").query('_merge == \"left_only\"')\n",
    "\n",
    "len(diff_1), len(diff_2)\n",
    "\n",
    "# no difference, means we are able to delete the error rows in the pr_full_data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_full_data_clean = pr_full_data[pr_full_data['error'].isna()].copy()\n",
    "pr_full_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_full_data_clean['files'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e3fa6",
   "metadata": {},
   "source": [
    "# Process Repo level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_full_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376eeb3",
   "metadata": {},
   "source": [
    "## Remove access blocked Github Repo and remove deleted pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31837dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: remove 451 repo access block repo from merged_pr_df, use html_url match the repo_url then remove\n",
    "# TODO: remove 404 pull requests, use html_url directly\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_pr_df = pd.read_parquet(\"merged_pr_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_pr_df)\n",
    "\n",
    "# group by repo and then sort all the pull request by merged_at, then we apply codeQL check on each pull request full codebase.\n",
    "# then we should be able to see, after x numbers of pull requests, how many issues are still there? increased? decreased?\n",
    "# run all the language that codeQL is supporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove access blocked html url\n",
    "err451 = error_full_data[\n",
    "    error_full_data[\"error\"].astype(str).str.contains(r\"\\b451\\b\", na=False)\n",
    "].copy()\n",
    "\n",
    "blocked_pr_urls = set(err451[\"html_url\"])\n",
    "\n",
    "blocked_repos = (\n",
    "    merged_pr_df.loc[merged_pr_df[\"html_url\"].isin(blocked_pr_urls), \"repo_url\"]\n",
    ")\n",
    "\n",
    "merged_pr_df_clean = merged_pr_df[~merged_pr_df[\"repo_url\"].isin(blocked_repos)].copy()\n",
    "display(merged_pr_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445555fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 404 not found pull request\n",
    "\n",
    "err404 = error_full_data[\n",
    "    error_full_data[\"error\"]\n",
    "    .astype(str)\n",
    "    .str.contains(r\"\\b404\\b\", na=False)\n",
    "].copy()\n",
    "\n",
    "err404_pr_urls = set(err404[\"html_url\"])\n",
    "\n",
    "merged_pr_df_clean_2 = merged_pr_df_clean[\n",
    "    ~merged_pr_df_clean[\"html_url\"].isin(err404_pr_urls)\n",
    "].copy()\n",
    "\n",
    "\n",
    "len(merged_pr_df), len(merged_pr_df_clean), len(merged_pr_df_clean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pr_df_clean_2['repo_url'].value_counts()\n",
    "# we have 1765 repositories in the original merged_pr_df\n",
    "# after we remove the 404 pull request and 451 repo\n",
    "# we still have 1752 repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed71db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pr_df_clean_2['repo_url'].value_counts()[lambda x: x >= 1]\n",
    "# we have xxxx repos can support this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_gt_1 = (\n",
    "    merged_pr_df_clean_2['repo_url']\n",
    "    .value_counts()\n",
    "    .loc[lambda x: x >= 1] # repo contains at least one pull request\n",
    "    .index\n",
    ")\n",
    "\n",
    "df_multi_repo = merged_pr_df_clean_2[\n",
    "    merged_pr_df_clean_2['repo_url'].isin(repos_gt_1)\n",
    "].copy()\n",
    "\n",
    "df_multi_repo['merged_at'] = pd.to_datetime(df_multi_repo['merged_at'])\n",
    "\n",
    "df_multi_repo = df_multi_repo.sort_values(\n",
    "    by=['repo_url', 'merged_at'],\n",
    "    ascending=[True, True]\n",
    ")\n",
    "\n",
    "df_multi_repo['pr_order_in_repo'] = (\n",
    "    df_multi_repo\n",
    "    .groupby('repo_url')\n",
    "    .cumcount()\n",
    "    + 1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi_repo\n",
    "\n",
    "# we have 23893 pull requests now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_pr_urls = (\n",
    "    df_multi_repo\n",
    "    .groupby('repo_url')['html_url']\n",
    "    .apply(list)\n",
    ")\n",
    "\n",
    "repo_pr_urls_dict = repo_pr_urls.to_dict()\n",
    "\n",
    "repo_pr_records = (\n",
    "    df_multi_repo\n",
    "    .groupby('repo_url')\n",
    "    .apply(\n",
    "        lambda g: g[[\n",
    "            'html_url',\n",
    "            'merged_at',\n",
    "            'pr_order_in_repo'\n",
    "        ]].to_dict(orient='records')\n",
    "    )\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "display(repo_pr_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then put the merge commit sha and html url together\n",
    "url_to_sha = (\n",
    "    pr_full_data_clean[['html_url', 'merge_commit_sha']]\n",
    "    .dropna(subset=['html_url', 'merge_commit_sha'])\n",
    "    .drop_duplicates(subset=['html_url'])\n",
    "    .set_index('html_url')['merge_commit_sha']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(url_to_sha['https://github.com/0x80/isolate-package/pull/132'])\n",
    "\n",
    "\n",
    "missing_sha = []\n",
    "\n",
    "for repo_url, pr_list in repo_pr_records.items():\n",
    "    for pr in pr_list:\n",
    "        html = pr['html_url']\n",
    "        sha = url_to_sha.get(html)\n",
    "\n",
    "        pr['merge_commit_sha'] = sha\n",
    "\n",
    "        if sha is None:\n",
    "            missing_sha.append(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35892cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_have_sha = all(\n",
    "    pr.get('merge_commit_sha') is not None\n",
    "    for repo_prs in repo_pr_records.values()\n",
    "    for pr in repo_prs\n",
    ")\n",
    "\n",
    "all_have_sha\n",
    "# all the pull requests have their own merged commit sha, we are able to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_pr_records.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_pr_records['https://api.github.com/repos/3rdIteration/btcrecover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f879c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_files = (\n",
    "    pr_full_data_clean[['html_url', 'files']]\n",
    "    .dropna(subset=['html_url'])\n",
    "    .drop_duplicates(subset=['html_url'])\n",
    "    .set_index('html_url')['files']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "missing_files = []\n",
    "\n",
    "for repo_url, pr_list in repo_pr_records.items():\n",
    "    for pr in pr_list:\n",
    "        html = pr['html_url']\n",
    "        files = url_to_files.get(html)\n",
    "\n",
    "        pr['files'] = files\n",
    "\n",
    "        if files is None:\n",
    "            missing_files.append(html)\n",
    "\n",
    "len(missing_files)\n",
    "\n",
    "# put files changed in the repo_pr_records, we can distinguish the files changed by agent by each pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c22dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "repo_pr_records['https://api.github.com/repos/3rdIteration/btcrecover'][0]['files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePosixPath\n",
    "\n",
    "# from codeql document: https://codeql.github.com/docs/codeql-overview/supported-languages-and-frameworks/\n",
    "# codeql only support these languages\n",
    "\n",
    "CODEQL_LANGUAGES = {\n",
    "    \"C/C++\",\n",
    "    \"C#\",\n",
    "    \"Go\",\n",
    "    \"Java/Kotlin\",\n",
    "    \"JavaScript\",\n",
    "    \"TypeScript\",\n",
    "    \"Python\",\n",
    "    \"Ruby\",\n",
    "    \"Rust\",\n",
    "    \"Swift\",\n",
    "    \"GitHub Actions\",\n",
    "}\n",
    "\n",
    "JS_EXTENSIONS = {\n",
    "    \".js\", \".jsx\", \".mjs\", \".es\", \".es6\",\n",
    "    \".htm\", \".html\", \".xhtm\", \".xhtml\",\n",
    "    \".vue\",\n",
    "    \".hbs\", \".ejs\", \".njk\",\n",
    "    \".json\",\n",
    "    \".yaml\", \".yml\",\n",
    "    \".raml\",\n",
    "    \".xml\",\n",
    "    \".ts\", \".tsx\", \".mts\", \".cts\"\n",
    "}\n",
    "\n",
    "# TS_EXTENSIONS = {\n",
    "#     \".ts\", \".tsx\", \".mts\", \".cts\"\n",
    "# }\n",
    "# ts goes into js as codeql does it\n",
    "\n",
    "CSHARP_EXTENSIONS = {\n",
    "    \".sln\",\n",
    "    \".slnx\",\n",
    "    \".csproj\",\n",
    "    \".cs\",\n",
    "    \".cshtml\",\n",
    "    \".xaml\",\n",
    "}\n",
    "\n",
    "\n",
    "RUBY_EXTENSIONS = {\n",
    "    \".rb\",        # Ruby source\n",
    "    \".erb\",       # Embedded Ruby (Rails views)\n",
    "    \".gemspec\",   # Gem specification\n",
    "}\n",
    "\n",
    "RUBY_FILENAMES = {\n",
    "    \"Gemfile\",\n",
    "}\n",
    "\n",
    "RUST_EXTENSIONS = {\n",
    "    \".rs\",        # Rust source\n",
    "}\n",
    "\n",
    "RUST_FILENAMES = {\n",
    "    \"Cargo.toml\", # Rust package manifest\n",
    "}\n",
    "\n",
    "\n",
    "def codeql_language_from_filename(filename: str) -> str:\n",
    "    p = PurePosixPath(filename)\n",
    "    ext = p.suffix.lower()\n",
    "    name = p.name\n",
    "    path = p.as_posix()\n",
    "\n",
    "    # github actions\n",
    "    if \".github/workflows\" in path and ext in {\".yml\", \".yaml\"}:\n",
    "        return \"Other\"\n",
    "\n",
    "    # JavaScript and Typescript\n",
    "    if ext in JS_EXTENSIONS:\n",
    "        return \"JavaScript/TypeScript\"\n",
    "    \n",
    "    # Python\n",
    "    if ext in {\".py\", \".pyi\"}:\n",
    "        return \"Python\"\n",
    "\n",
    "    # Java / Kotlin\n",
    "    if ext in {\".java\", \".kt\"}:\n",
    "        return \"Java/Kotlin\"\n",
    "\n",
    "    # C / C++\n",
    "    if ext in {\".c\", \".h\", \".cc\", \".cpp\", \".cxx\", \".hpp\"}:\n",
    "        return \"C/C++\"\n",
    "\n",
    "    # C#\n",
    "    if ext in CSHARP_EXTENSIONS:\n",
    "        return \"C#\"\n",
    "    \n",
    "    # Ruby\n",
    "    if ext in RUBY_EXTENSIONS or name in RUBY_FILENAMES:\n",
    "        return \"Ruby\"\n",
    "\n",
    "    # Rust\n",
    "    if ext in RUST_EXTENSIONS or name in RUST_FILENAMES:\n",
    "        return \"Rust\"\n",
    "\n",
    "    # # Go\n",
    "    # if ext == \".go\":\n",
    "    #     return \"Go\"\n",
    "\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "def extract_codeql_languages_from_pr(files: list[dict]) -> list[str]:\n",
    "    return sorted({\n",
    "        codeql_language_from_filename(f[\"filename\"])\n",
    "        for f in files or []\n",
    "        if \"filename\" in f\n",
    "    })\n",
    "\n",
    "def apply_codeql_languages_to_all_prs(repo_pr_records: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Mutates repo_pr_records in-place.\n",
    "    For each PR, adds a `language` field:\n",
    "        pr[\"language\"] = list of CodeQL languages\n",
    "    \"\"\"\n",
    "    for repo_url, pr_list in repo_pr_records.items():\n",
    "        for pr in pr_list:\n",
    "            files = pr.get(\"files\", [])\n",
    "            pr[\"language\"] = extract_codeql_languages_from_pr(files)\n",
    "    return repo_pr_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_pr_records = apply_codeql_languages_to_all_prs(repo_pr_records)\n",
    "# get languages for each pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_pr_records['https://api.github.com/repos/0x4D31/galah'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_languages(repo_pr_records: dict) -> set[str]:\n",
    "    all_languages = set()\n",
    "\n",
    "    for pr_list in repo_pr_records.values():\n",
    "        for pr in pr_list:\n",
    "            for lang in pr.get(\"language\", []):\n",
    "                all_languages.add(lang)\n",
    "\n",
    "    return all_languages\n",
    "\n",
    "all_languages = get_all_languages(repo_pr_records)\n",
    "\n",
    "print(len(all_languages))\n",
    "print(sorted(all_languages))\n",
    "# there are two languages appeared in the pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_only_other_prs(repo_pr_records: dict) -> int:\n",
    "    count = 0\n",
    "\n",
    "    for pr_list in repo_pr_records.values():\n",
    "        for pr in pr_list:\n",
    "            langs = pr.get(\"language\", [])\n",
    "            if langs and all(lang == \"Other\" for lang in langs):\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "# now we need to check the pull request only contains other language, we need to remove them\n",
    "\n",
    "only_other_pr_count = count_only_other_prs(repo_pr_records)\n",
    "print(\"PRs with only 'Other':\", only_other_pr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_only_other_prs(repo_pr_records: dict) -> dict:\n",
    "    cleaned = {}\n",
    "\n",
    "    for repo_url, pr_list in repo_pr_records.items():\n",
    "        new_pr_list = []\n",
    "\n",
    "        for pr in pr_list:\n",
    "            langs = pr.get(\"language\", [])\n",
    "            if not (langs and all(lang == \"Other\" for lang in langs)):\n",
    "                new_pr_list.append(pr)\n",
    "\n",
    "        if new_pr_list:\n",
    "            cleaned[repo_url] = new_pr_list\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "repo_pr_records_clean = remove_only_other_prs(repo_pr_records)\n",
    "\n",
    "print(\"Before cleanup:\")\n",
    "print(\"Repos:\", len(repo_pr_records))\n",
    "print(\"PRs:\", sum(len(v) for v in repo_pr_records.values()))\n",
    "print(\"Only-Other PRs:\", count_only_other_prs(repo_pr_records))\n",
    "\n",
    "repo_pr_records = remove_only_other_prs(repo_pr_records)\n",
    "\n",
    "print(\"\\nAfter cleanup:\")\n",
    "print(\"Repos:\", len(repo_pr_records))\n",
    "print(\"PRs:\", sum(len(v) for v in repo_pr_records.values()))\n",
    "print(\"Only-Other PRs:\", count_only_other_prs(repo_pr_records))\n",
    "\n",
    "# after we remove the other only pull request, we have  repos remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# os.makedirs(\"artifacts\", exist_ok=True)\n",
    "# output_path = \"artifacts/repo_pr_records_clean.json\"\n",
    "\n",
    "# def json_serializer(obj):\n",
    "#     if isinstance(obj, (pd.Timestamp, datetime)):\n",
    "#         return obj.isoformat()\n",
    "#     return str(obj)\n",
    "\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(\n",
    "#         repo_pr_records_clean,\n",
    "#         f,\n",
    "#         indent=2,\n",
    "#         ensure_ascii=False,\n",
    "#         default=json_serializer\n",
    "#     )\n",
    "\n",
    "# print(f\"‚úÖ Saved to {output_path}\")\n",
    "# this file is 1.5 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_pr_records_clean['https://api.github.com/repos/XiangpengHao/liquid-cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b04c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have cleaned the data and all the repos and pull request within valid repos\n",
    "# next we should be able to run codeql on the entire codebase for that specific merged commit and then we check wether the issues are introduced by agent\n",
    "\n",
    "# we only analyse the language in the pull request's language list, in this way we have a high chance to find wether the issue is introduced by agent \n",
    "# and faster for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm trying apply codeQL on the each pull request for the repo\n",
    "\n",
    "# step1: extract data\n",
    "def extract_repo_records(api_url, pr_list):\n",
    "\n",
    "    repo_url = api_url.replace(\"https://api.github.com\", \"https://github.com\")\n",
    "    if \"/repos/\" in repo_url:\n",
    "        repo_url = repo_url.replace(\"/repos/\", \"/\")\n",
    "        \n",
    "    records = []\n",
    "    for item in pr_list:\n",
    "        print(item)\n",
    "        records.append({\n",
    "            'html_url': item['html_url'],\n",
    "            'merge_commit_sha': item['merge_commit_sha'],\n",
    "            'pr_order': item['pr_order_in_repo'],\n",
    "            'language': item['language']\n",
    "        })\n",
    "    return repo_url, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0954042",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODEQL_QUERY_SUITES = {\n",
    "    \"C/C++\": \"cpp-security-and-quality.qls\",\n",
    "    \"C#\": \"csharp-security-and-quality.qls\",\n",
    "    \"Java/Kotlin\": \"java-security-and-quality.qls\",\n",
    "    \"JavaScript/TypeScript\": \"javascript-security-and-quality.qls\",\n",
    "    \"Python\": \"python-security-and-quality.qls\",\n",
    "    \"Ruby\": \"ruby-security-and-quality.qls\",\n",
    "    \"Rust\": \"rust-security-and-quality.qls\",\n",
    "}\n",
    "\n",
    "CODEQL_CLI_LANG_MAP = {\n",
    "    \"C/C++\": \"cpp\",\n",
    "    \"C#\": \"csharp\",\n",
    "    \"Java/Kotlin\": \"java\",\n",
    "    \"JavaScript/TypeScript\": \"javascript\",\n",
    "    \"Python\": \"python\",\n",
    "    \"Ruby\": \"ruby\",\n",
    "    \"Rust\": \"rust\",\n",
    "}\n",
    "\n",
    "CODEQL_BUILD_MODE = {\n",
    "    \"C/C++\": \"none\",\n",
    "    \"C#\": \"none\",\n",
    "    \"Java/Kotlin\": \"none\",\n",
    "    \"JavaScript/TypeScript\": \"none\",\n",
    "    \"Python\": \"none\",\n",
    "    \"Ruby\": \"none\",\n",
    "    \"Rust\": \"none\",\n",
    "}\n",
    "\n",
    "def select_build_mode(pr_languages: list[str]) -> str:\n",
    "    for lang in pr_languages:\n",
    "        if CODEQL_BUILD_MODE.get(lang) == \"none\":\n",
    "            return \"none\"\n",
    "    return \"autobuild\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scan_pipeline(repo_url, records):\n",
    "    import subprocess\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    CODEQL_BIN = \"/Users/guanhao/codeql-home/codeql/codeql\"\n",
    "    BASE_REPORT_DIR = \"codeql-reports\"\n",
    "\n",
    "    parts = repo_url.rstrip(\"/\").split(\"/\")\n",
    "    repo_folder_name = f\"{parts[-2]}_{parts[-1]}\"\n",
    "\n",
    "    for pr in records:\n",
    "        sha = pr[\"merge_commit_sha\"]\n",
    "        pr_id = pr[\"pr_order\"]\n",
    "        pr_languages = pr.get(\"language\", [])\n",
    "\n",
    "        pr_langs = [\n",
    "            l for l in pr_languages\n",
    "            if l in CODEQL_CLI_LANG_MAP and l != \"GitHub Actions\"\n",
    "        ]\n",
    "\n",
    "        if not pr_langs:\n",
    "            print(f\"\\n‚è≠Ô∏è  [PR {pr_id}] Skipped (no CodeQL-supported languages)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüöÄ [PR {pr_id}] SHA: {sha}\")\n",
    "        print(f\"   üß† PR languages (CodeQL): {pr_langs}\")\n",
    "\n",
    "        report_dir = os.path.join(\n",
    "            BASE_REPORT_DIR, repo_folder_name, f\"PR_{pr_id}\"\n",
    "        )\n",
    "        os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "        source_dir = f\"temp_source_pr_{pr_id}\"\n",
    "        os.makedirs(source_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # ===== checkout merge commit (once per PR) =====\n",
    "            subprocess.run(\n",
    "                f\"git init && \"\n",
    "                f\"git remote add origin {repo_url} && \"\n",
    "                f\"git fetch --depth 1 origin {sha} && \"\n",
    "                f\"git checkout FETCH_HEAD\",\n",
    "                shell=True,\n",
    "                cwd=source_dir,\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "            )\n",
    "\n",
    "            # ===== language-level isolation =====\n",
    "            for pr_lang in pr_langs:\n",
    "                cli_lang = CODEQL_CLI_LANG_MAP[pr_lang]\n",
    "                query_suite = CODEQL_QUERY_SUITES[pr_lang]\n",
    "\n",
    "                db_dir = f\"temp_db_pr_{pr_id}_{cli_lang}\"\n",
    "                db_dir_abs = os.path.abspath(db_dir)\n",
    "\n",
    "                print(f\"\\n   üèóÔ∏è  [{pr_lang}] Creating DB ({cli_lang})\")\n",
    "\n",
    "                build_mode = CODEQL_BUILD_MODE.get(pr_lang, \"none\")\n",
    "\n",
    "                try:\n",
    "                    # --- DB create ---\n",
    "                    subprocess.run(\n",
    "                        f\"{CODEQL_BIN} database create {db_dir_abs} \"\n",
    "                        f\"--source-root=. \"\n",
    "                        f\"--language={cli_lang} \"\n",
    "                        f\"--build-mode={build_mode}\",\n",
    "                        shell=True,\n",
    "                        cwd=source_dir,\n",
    "                        check=True,\n",
    "                    )\n",
    "\n",
    "                    output_path = os.path.join(\n",
    "                        report_dir,\n",
    "                        f\"{cli_lang}_security_report.sarif\"\n",
    "                    )\n",
    "\n",
    "                    print(f\"   üîç [{pr_lang}] Analyzing...\")\n",
    "\n",
    "                    # --- analyze ---\n",
    "                    subprocess.run(\n",
    "                        f\"{CODEQL_BIN} database analyze \"\n",
    "                        f\"{db_dir_abs} \"\n",
    "                        f\"{query_suite} \"\n",
    "                        f\"--format=sarif-latest \"\n",
    "                        f\"--output={os.path.abspath(output_path)}\",\n",
    "                        shell=True,\n",
    "                        check=True,\n",
    "                    )\n",
    "\n",
    "                    print(f\"   ‚úÖ [{pr_lang}] Report: {output_path}\")\n",
    "\n",
    "                except Exception as lang_err:\n",
    "                    # ‚ö†Ô∏è language-level failure is tolerated\n",
    "                    print(\n",
    "                        f\"   ‚ö†Ô∏è  [{pr_lang}] Failed, skipped.\\n\"\n",
    "                        f\"       Reason: {lang_err}\"\n",
    "                    )\n",
    "\n",
    "                finally:\n",
    "                    # --- always clean DB dir ---\n",
    "                    if os.path.exists(db_dir_abs):\n",
    "                        shutil.rmtree(db_dir_abs)\n",
    "\n",
    "        except Exception as pr_err:\n",
    "            # ‚ùå only catastrophic PR-level failures land here\n",
    "            print(f\"   ‚ùå Error preparing PR {pr_id}: {pr_err}\")\n",
    "\n",
    "        finally:\n",
    "            print(\"   üßπ Cleaning up source workspace...\")\n",
    "            if os.path.exists(source_dir):\n",
    "                shutil.rmtree(source_dir)\n",
    "\n",
    "    print(f\"\\n‚ú® All scans for {repo_folder_name} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested: rust, java\\kotlin, c\\c++, python, js\\ts, c#, ruby\n",
    "# we can use these languages\n",
    "\n",
    "# full pipeline get codeql report\n",
    "\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "BASE_REPORT_DIR = \"codeql-reports\"\n",
    "\n",
    "os.makedirs(BASE_REPORT_DIR, exist_ok=True)\n",
    "\n",
    "total = len(repo_pr_records_clean)\n",
    "done = 0\n",
    "skipped = 0\n",
    "failed = 0\n",
    "\n",
    "for idx, (api_repo_url, pr_records) in enumerate(repo_pr_records_clean.items(), start=1):\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ [{idx}/{total}] Processing repo:\")\n",
    "    print(f\"    {api_repo_url}\")\n",
    "\n",
    "    # ===== API url ‚Üí GitHub url & records =====\n",
    "    repo_url, records = extract_repo_records(api_repo_url, pr_records)\n",
    "\n",
    "    parts = repo_url.rstrip(\"/\").split(\"/\")\n",
    "    repo_folder_name = f\"{parts[-2]}_{parts[-1]}\"\n",
    "    repo_report_path = os.path.join(BASE_REPORT_DIR, repo_folder_name)\n",
    "    done_marker = os.path.join(repo_report_path, \"_DONE\")\n",
    "\n",
    "    # ===== skip if fully completed =====\n",
    "    if os.path.exists(done_marker):\n",
    "        print(f\"‚è≠Ô∏è  Skipped (already completed): {repo_folder_name}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    print(f\"üöÄ Running CodeQL for {repo_folder_name}\")\n",
    "    os.makedirs(repo_report_path, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        run_scan_pipeline(repo_url, records)\n",
    "\n",
    "        # ===== mark repo as completed =====\n",
    "        with open(done_marker, \"w\") as f:\n",
    "            f.write(\"ok\\n\")\n",
    "\n",
    "        print(f\"‚úÖ Completed: {repo_folder_name}\")\n",
    "        done += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        print(f\"‚ùå Failed: {repo_folder_name}\")\n",
    "        print(\"   Reason:\", str(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ Batch finished\")\n",
    "print(f\"‚úÖ Completed repos : {done}\")\n",
    "print(f\"‚è≠Ô∏è  Skipped repos   : {skipped}\")\n",
    "print(f\"‚ùå Failed repos    : {failed}\")\n",
    "print(f\"üì¶ Total repos     : {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good job Hao!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75382f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= imports =========\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import subprocess\n",
    "import shutil\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# ========= SARIF parsing =========\n",
    "def parse_sarif_findings(sarif_path: str) -> list[dict]:\n",
    "    with open(sarif_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sarif = json.load(f)\n",
    "\n",
    "    findings = []\n",
    "    for run in sarif.get(\"runs\", []):\n",
    "        for r in run.get(\"results\", []) or []:\n",
    "            rule_id = r.get(\"ruleId\") or (r.get(\"rule\") or {}).get(\"id\")\n",
    "            message = (r.get(\"message\") or {}).get(\"text\", \"\")\n",
    "            level = r.get(\"level\")\n",
    "\n",
    "            locs = r.get(\"locations\") or []\n",
    "            if not locs:\n",
    "                continue\n",
    "\n",
    "            pl = locs[0].get(\"physicalLocation\") or {}\n",
    "            artifact = (pl.get(\"artifactLocation\") or {}).get(\"uri\")\n",
    "            region = pl.get(\"region\") or {}\n",
    "            start_line = region.get(\"startLine\")\n",
    "\n",
    "            if not artifact or not start_line:\n",
    "                continue\n",
    "\n",
    "            findings.append({\n",
    "                \"rule_id\": rule_id,\n",
    "                \"message\": message,\n",
    "                \"level\": level,\n",
    "                \"file\": artifact,\n",
    "                \"start_line\": int(start_line),\n",
    "            })\n",
    "\n",
    "    return findings\n",
    "\n",
    "\n",
    "# ========= path normalization =========\n",
    "def normalize_sarif_path(uri: str, repo_root_abs: str) -> str:\n",
    "    if uri.startswith(\"file://\"):\n",
    "        path = urlparse(uri).path\n",
    "    else:\n",
    "        path = uri\n",
    "\n",
    "    path = os.path.normpath(path).lstrip(\"./\")\n",
    "\n",
    "    if os.path.isabs(path):\n",
    "        repo_root_abs = os.path.normpath(repo_root_abs)\n",
    "        if path.startswith(repo_root_abs):\n",
    "            path = os.path.relpath(path, repo_root_abs)\n",
    "\n",
    "    return path.lstrip(\"./\")\n",
    "\n",
    "\n",
    "# ========= introduced-by-PR marking =========\n",
    "def mark_introduced(findings, added_lines_by_file, repo_root_abs):\n",
    "    out = []\n",
    "    for f in findings:\n",
    "        file_path = normalize_sarif_path(f[\"file\"], repo_root_abs)\n",
    "        line = f[\"start_line\"]\n",
    "\n",
    "        introduced = line in added_lines_by_file.get(file_path, set())\n",
    "\n",
    "        f2 = dict(f)\n",
    "        f2[\"normalized_file\"] = file_path\n",
    "        f2[\"introduced_by_pr\"] = introduced\n",
    "        out.append(f2)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ========= diff parsing =========\n",
    "HUNK_RE = re.compile(r\"^\\@\\@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? \\@\\@\")\n",
    "\n",
    "def get_added_lines_by_file(repo_dir: str, merge_sha: str) -> dict[str, set[int]]:\n",
    "    cmd = [\"git\", \"diff\", \"-U0\", f\"{merge_sha}^1\", merge_sha]\n",
    "    p = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=repo_dir,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    added = defaultdict(set)\n",
    "    current_file = None\n",
    "    new_line = None\n",
    "\n",
    "    for line in p.stdout.splitlines():\n",
    "        if line.startswith(\"+++ b/\"):\n",
    "            current_file = line[len(\"+++ b/\"):].strip().lstrip(\"./\")\n",
    "            new_line = None\n",
    "            continue\n",
    "\n",
    "        m = HUNK_RE.match(line)\n",
    "        if m:\n",
    "            new_line = int(m.group(1))\n",
    "            continue\n",
    "\n",
    "        if current_file is None or new_line is None:\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"+\") and not line.startswith(\"+++\"):\n",
    "            added[current_file].add(new_line)\n",
    "            new_line += 1\n",
    "        elif line.startswith(\"-\") and not line.startswith(\"---\"):\n",
    "            continue\n",
    "        else:\n",
    "            new_line += 1\n",
    "\n",
    "    return dict(added)\n",
    "\n",
    "def api_to_clone_url(api_url: str) -> str:\n",
    "    return api_url.replace(\n",
    "        \"https://api.github.com/repos/\",\n",
    "        \"https://github.com/\"\n",
    "    )\n",
    "\n",
    "\n",
    "def repo_folder_to_api_url(repo_folder: str) -> str:\n",
    "    # BOINC_boinc -> https://api.github.com/repos/BOINC/boinc\n",
    "    owner, repo = repo_folder.split(\"_\", 1)\n",
    "    return f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "\n",
    "def write_jsonl(path: str, rows: list[dict]):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcf744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_REPORTS = \"codeql-reports\"\n",
    "# OUTPUT_BASE = \"issues-introduced-by-agent\"\n",
    "# TMP_REPO = \"_tmp_repo\"\n",
    "\n",
    "# os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "# for repo_folder in sorted(os.listdir(BASE_REPORTS)):\n",
    "#     repo_path = os.path.join(BASE_REPORTS, repo_folder)\n",
    "#     if not os.path.isdir(repo_path):\n",
    "#         continue\n",
    "\n",
    "#     print(f\"\\nüì¶ Repo: {repo_folder}\")\n",
    "\n",
    "#     api_repo = repo_folder_to_api_url(repo_folder)\n",
    "#     if api_repo not in repo_pr_records_clean:\n",
    "#         print(\"  ‚ö†Ô∏è Repo not found in records, skipped\")\n",
    "#         continue\n",
    "\n",
    "#     clone_url = api_to_clone_url(api_repo)\n",
    "#     pr_records = repo_pr_records_clean[api_repo]\n",
    "\n",
    "#     # PR order -> record\n",
    "#     pr_by_order = {\n",
    "#         p[\"pr_order_in_repo\"]: p\n",
    "#         for p in pr_records\n",
    "#         if p.get(\"merge_commit_sha\")\n",
    "#     }\n",
    "\n",
    "#     for pr_dir in sorted(os.listdir(repo_path)):\n",
    "#         if not pr_dir.startswith(\"PR_\"):\n",
    "#             continue\n",
    "\n",
    "#         pr_order = int(pr_dir.replace(\"PR_\", \"\"))\n",
    "#         print(f\"  üîπ PR {pr_order}\")\n",
    "\n",
    "#         if pr_order not in pr_by_order:\n",
    "#             print(\"    ‚ö†Ô∏è PR not in records, skipped\")\n",
    "#             continue\n",
    "\n",
    "#         pr_record = pr_by_order[pr_order]\n",
    "#         merge_sha = pr_record[\"merge_commit_sha\"]\n",
    "\n",
    "#         sarif_dir = os.path.join(repo_path, pr_dir)\n",
    "#         sarif_files = glob.glob(os.path.join(sarif_dir, \"*.sarif\"))\n",
    "#         if not sarif_files:\n",
    "#             print(\"    ‚ö†Ô∏è No SARIF files, skipped\")\n",
    "#             continue\n",
    "\n",
    "#         # ----- temp repo -----\n",
    "#         if os.path.exists(TMP_REPO):\n",
    "#             shutil.rmtree(TMP_REPO)\n",
    "#         os.makedirs(TMP_REPO, exist_ok=True)\n",
    "\n",
    "#         try:\n",
    "#             # checkout\n",
    "#             subprocess.run(\n",
    "#                 f\"git init && \"\n",
    "#                 f\"git remote add origin {clone_url} && \"\n",
    "#                 f\"git fetch --depth 2 origin {merge_sha} && \"\n",
    "#                 f\"git checkout {merge_sha}\",\n",
    "#                 shell=True,\n",
    "#                 cwd=TMP_REPO,\n",
    "#                 check=True,\n",
    "#                 stdout=subprocess.DEVNULL,\n",
    "#                 stderr=subprocess.DEVNULL,\n",
    "#             )\n",
    "\n",
    "#             # ensure parent\n",
    "#             subprocess.run(\n",
    "#                 [\"git\", \"rev-parse\", f\"{merge_sha}^1\"],\n",
    "#                 cwd=TMP_REPO,\n",
    "#                 check=True,\n",
    "#                 stdout=subprocess.DEVNULL,\n",
    "#             )\n",
    "\n",
    "#             # diff\n",
    "#             added_lines = get_added_lines_by_file(TMP_REPO, merge_sha)\n",
    "\n",
    "#             all_marked = []\n",
    "#             all_introduced = []\n",
    "\n",
    "#             for sarif_path in sarif_files:\n",
    "#                 findings = parse_sarif_findings(sarif_path)\n",
    "#                 marked = mark_introduced(\n",
    "#                     findings,\n",
    "#                     added_lines,\n",
    "#                     os.path.abspath(TMP_REPO)\n",
    "#                 )\n",
    "#                 all_marked.extend(marked)\n",
    "#                 all_introduced.extend(\n",
    "#                     f for f in marked if f[\"introduced_by_pr\"]\n",
    "#                 )\n",
    "\n",
    "#             # ----- save -----\n",
    "#             out_dir = os.path.join(OUTPUT_BASE, repo_folder, pr_dir)\n",
    "#             os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#             write_jsonl(os.path.join(out_dir, \"findings_all.jsonl\"), all_marked)\n",
    "#             write_jsonl(os.path.join(out_dir, \"findings_introduced.jsonl\"), all_introduced)\n",
    "\n",
    "#             summary = {\n",
    "#                 \"repo\": api_repo,\n",
    "#                 \"repo_folder\": repo_folder,\n",
    "#                 \"pr_order_in_repo\": pr_order,\n",
    "#                 \"pr_html_url\": pr_record[\"html_url\"],\n",
    "#                 \"merge_sha\": merge_sha,\n",
    "#                 \"sarif_files\": [os.path.basename(p) for p in sarif_files],\n",
    "#                 \"total_findings\": len(all_marked),\n",
    "#                 \"introduced_findings\": len(all_introduced),\n",
    "#                 \"pr_author_type\": \"agent\",\n",
    "#             }\n",
    "\n",
    "#             with open(os.path.join(out_dir, \"summary.json\"), \"w\") as f:\n",
    "#                 json.dump(summary, f, indent=2)\n",
    "\n",
    "#             print(f\"    ‚úÖ Saved ({len(all_introduced)} introduced)\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"    ‚ùå Error: {e}\")\n",
    "\n",
    "#         finally:\n",
    "#             if os.path.exists(TMP_REPO):\n",
    "#                 shutil.rmtree(TMP_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good job! Hao!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccafc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # full pipeline checkout issues actually introduced by agent\n",
    "\n",
    "# repo_pr_records_clean['https://api.github.com/repos/OpenHFT/Chronicle-Core']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
